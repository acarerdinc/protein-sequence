{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Missing proteins: ', 174856)\n",
      "('All count: ', 184939)\n",
      "('Number of samples:', 3690)\n",
      "('Number of unique input tokens:', 21)\n",
      "('Number of unique output tokens:', 23)\n",
      "('Max sequence length for inputs:', 350)\n",
      "('Max sequence length for outputs:', 352)\n",
      "('Class counts:', [('1', 857), ('3', 1028), ('2', 717), ('5', 85), ('4', 313), ('6', 690)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Masking, Bidirectional\n",
    "from keras.regularizers import l2, l1\n",
    "import numpy as np\n",
    "from readdata import read_data\n",
    "import collections\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "save_dir = 'saves'\n",
    "\n",
    "x_train, y_train, x_text, y_test = read_data(level=0, length_limit=350)\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "for i, (input_text, target) in enumerate(zip(x_train, y_train)):\n",
    "    input_texts.append(input_text)\n",
    "    target_text = '\\t' + input_text + '\\n'\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "print('Class counts:', [(key, value) for\n",
    "                        key, value in (collections.Counter(y_train)).iteritems()])\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, -(t+1), input_token_index[char]] = 1.   #-(t+1)\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3690, 350, 21)\n",
      "(3690, 352, 23)\n",
      "(3690, 352, 23)\n"
     ]
    }
   ],
   "source": [
    "print encoder_input_data.shape\n",
    "print decoder_input_data.shape\n",
    "print decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2952 samples, validate on 738 samples\n",
      "Epoch 1/200\n",
      "2952/2952 [==============================] - 43s - loss: 2.2941 - val_loss: 1.7873\n",
      "Epoch 2/200\n",
      "2952/2952 [==============================] - 42s - loss: 2.1936 - val_loss: 1.7255\n",
      "Epoch 3/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.1583 - val_loss: 1.7120\n",
      "Epoch 4/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.1481 - val_loss: 1.7082\n",
      "Epoch 5/200\n",
      "2952/2952 [==============================] - 42s - loss: 2.1441 - val_loss: 1.7061\n",
      "Epoch 6/200\n",
      "2952/2952 [==============================] - 42s - loss: 2.1412 - val_loss: 1.7049\n",
      "Epoch 7/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.1378 - val_loss: 1.7045\n",
      "Epoch 8/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.1351 - val_loss: 1.7017\n",
      "Epoch 9/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.1324 - val_loss: 1.6998\n",
      "Epoch 10/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.1297 - val_loss: 1.6997\n",
      "Epoch 11/200\n",
      "2952/2952 [==============================] - 42s - loss: 2.1271 - val_loss: 1.6972\n",
      "Epoch 12/200\n",
      "2952/2952 [==============================] - 42s - loss: 2.1247 - val_loss: 1.6956\n",
      "Epoch 13/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.1223 - val_loss: 1.6937\n",
      "Epoch 14/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.1200 - val_loss: 1.6955\n",
      "Epoch 15/200\n",
      "2952/2952 [==============================] - 44s - loss: 2.1184 - val_loss: 1.6916\n",
      "Epoch 16/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.1155 - val_loss: 1.6905\n",
      "Epoch 17/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.1147 - val_loss: 1.6886\n",
      "Epoch 18/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.1123 - val_loss: 1.6908\n",
      "Epoch 19/200\n",
      "2952/2952 [==============================] - 42s - loss: 2.1112 - val_loss: 1.6887\n",
      "Epoch 20/200\n",
      "2952/2952 [==============================] - 42s - loss: 2.1094 - val_loss: 1.6884\n",
      "Epoch 21/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.1082 - val_loss: 1.6862\n",
      "Epoch 22/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.1064 - val_loss: 1.6858\n",
      "Epoch 23/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.1050 - val_loss: 1.6860\n",
      "Epoch 24/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.1038 - val_loss: 1.6870\n",
      "Epoch 25/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.1029 - val_loss: 1.6855\n",
      "Epoch 26/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.1013 - val_loss: 1.6853\n",
      "Epoch 27/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.1004 - val_loss: 1.6857\n",
      "Epoch 28/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0994 - val_loss: 1.6859\n",
      "Epoch 29/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0980 - val_loss: 1.6848\n",
      "Epoch 30/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0961 - val_loss: 1.6856\n",
      "Epoch 31/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0958 - val_loss: 1.6857\n",
      "Epoch 32/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0945 - val_loss: 1.6854\n",
      "Epoch 33/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0934 - val_loss: 1.6841\n",
      "Epoch 34/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0919 - val_loss: 1.6842\n",
      "Epoch 35/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0908 - val_loss: 1.6840\n",
      "Epoch 36/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0895 - val_loss: 1.6834\n",
      "Epoch 37/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0886 - val_loss: 1.6823\n",
      "Epoch 38/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0877 - val_loss: 1.6829\n",
      "Epoch 39/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0862 - val_loss: 1.6848\n",
      "Epoch 40/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0856 - val_loss: 1.6819\n",
      "Epoch 41/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0841 - val_loss: 1.6832\n",
      "Epoch 42/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0839 - val_loss: 1.6834\n",
      "Epoch 43/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0821 - val_loss: 1.6818\n",
      "Epoch 44/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0813 - val_loss: 1.6813\n",
      "Epoch 45/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0805 - val_loss: 1.6824\n",
      "Epoch 46/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0791 - val_loss: 1.6820\n",
      "Epoch 47/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0782 - val_loss: 1.6814\n",
      "Epoch 48/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0771 - val_loss: 1.6802\n",
      "Epoch 49/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0766 - val_loss: 1.6868\n",
      "Epoch 50/200\n",
      "2952/2952 [==============================] - 36s - loss: 2.0783 - val_loss: 1.6807\n",
      "Epoch 51/200\n",
      "2952/2952 [==============================] - 35s - loss: 2.0756 - val_loss: 1.6823\n",
      "Epoch 52/200\n",
      "2952/2952 [==============================] - 34s - loss: 2.0744 - val_loss: 1.6808\n",
      "Epoch 53/200\n",
      "2952/2952 [==============================] - 34s - loss: 2.0732 - val_loss: 1.6808\n",
      "Epoch 54/200\n",
      "2952/2952 [==============================] - 34s - loss: 2.0724 - val_loss: 1.6800\n",
      "Epoch 55/200\n",
      "2952/2952 [==============================] - 33s - loss: 2.0710 - val_loss: 1.6834\n",
      "Epoch 56/200\n",
      "2952/2952 [==============================] - 33s - loss: 2.0712 - val_loss: 1.6824\n",
      "Epoch 57/200\n",
      "2952/2952 [==============================] - 33s - loss: 2.0710 - val_loss: 1.6806\n",
      "Epoch 58/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0688 - val_loss: 1.6799\n",
      "Epoch 59/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0680 - val_loss: 1.6787\n",
      "Epoch 60/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0666 - val_loss: 1.6792\n",
      "Epoch 61/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0659 - val_loss: 1.6796\n",
      "Epoch 62/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0644 - val_loss: 1.6778\n",
      "Epoch 63/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0642 - val_loss: 1.6802\n",
      "Epoch 64/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0633 - val_loss: 1.6782\n",
      "Epoch 65/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0623 - val_loss: 1.6778\n",
      "Epoch 66/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0608 - val_loss: 1.6769\n",
      "Epoch 67/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0612 - val_loss: 1.6777\n",
      "Epoch 68/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0592 - val_loss: 1.6768\n",
      "Epoch 69/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0580 - val_loss: 1.6771\n",
      "Epoch 70/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0576 - val_loss: 1.6793\n",
      "Epoch 71/200\n",
      "2952/2952 [==============================] - 71s - loss: 2.0576 - val_loss: 1.6791\n",
      "Epoch 72/200\n",
      "2952/2952 [==============================] - 50s - loss: 2.0562 - val_loss: 1.6768\n",
      "Epoch 73/200\n",
      "2952/2952 [==============================] - 43s - loss: 2.0556 - val_loss: 1.6778\n",
      "Epoch 74/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0540 - val_loss: 1.6766\n",
      "Epoch 75/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0539 - val_loss: 1.6773\n",
      "Epoch 76/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0535 - val_loss: 1.6757\n",
      "Epoch 77/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0511 - val_loss: 1.6755\n",
      "Epoch 78/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0506 - val_loss: 1.6757\n",
      "Epoch 79/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0499 - val_loss: 1.6773\n",
      "Epoch 80/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0495 - val_loss: 1.6781\n",
      "Epoch 81/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0480 - val_loss: 1.6754\n",
      "Epoch 82/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0466 - val_loss: 1.6739\n",
      "Epoch 83/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0461 - val_loss: 1.6744\n",
      "Epoch 84/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0451 - val_loss: 1.6767\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2952/2952 [==============================] - 39s - loss: 2.0464 - val_loss: 1.6751\n",
      "Epoch 86/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0437 - val_loss: 1.6740\n",
      "Epoch 87/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0427 - val_loss: 1.6736\n",
      "Epoch 88/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0418 - val_loss: 1.6741\n",
      "Epoch 89/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0411 - val_loss: 1.6734\n",
      "Epoch 90/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0400 - val_loss: 1.6758\n",
      "Epoch 91/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0412 - val_loss: 1.6732\n",
      "Epoch 92/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0390 - val_loss: 1.6740\n",
      "Epoch 93/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0377 - val_loss: 1.6738\n",
      "Epoch 94/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0377 - val_loss: 1.6751\n",
      "Epoch 95/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0377 - val_loss: 1.6717\n",
      "Epoch 96/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0354 - val_loss: 1.6710\n",
      "Epoch 97/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0337 - val_loss: 1.6709\n",
      "Epoch 98/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0333 - val_loss: 1.6705\n",
      "Epoch 99/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0319 - val_loss: 1.6730\n",
      "Epoch 100/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0335 - val_loss: 1.6709\n",
      "Epoch 101/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0314 - val_loss: 1.6717\n",
      "Epoch 102/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0297 - val_loss: 1.6696\n",
      "Epoch 103/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0291 - val_loss: 1.6694\n",
      "Epoch 104/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0316 - val_loss: 1.6697\n",
      "Epoch 105/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0273 - val_loss: 1.6688\n",
      "Epoch 106/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0268 - val_loss: 1.6697\n",
      "Epoch 107/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0257 - val_loss: 1.6686\n",
      "Epoch 108/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0268 - val_loss: 1.6700\n",
      "Epoch 109/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0244 - val_loss: 1.6960\n",
      "Epoch 110/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0327 - val_loss: 1.6712\n",
      "Epoch 111/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0241 - val_loss: 1.6705\n",
      "Epoch 112/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0262 - val_loss: 1.6703\n",
      "Epoch 113/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0233 - val_loss: 1.6696\n",
      "Epoch 114/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0214 - val_loss: 1.6700\n",
      "Epoch 115/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0211 - val_loss: 1.6699\n",
      "Epoch 116/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0187 - val_loss: 1.6684\n",
      "Epoch 117/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0180 - val_loss: 1.6686\n",
      "Epoch 118/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0167 - val_loss: 1.6657\n",
      "Epoch 119/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0161 - val_loss: 1.6707\n",
      "Epoch 120/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0164 - val_loss: 1.6677\n",
      "Epoch 121/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0163 - val_loss: 1.6672\n",
      "Epoch 122/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0162 - val_loss: 1.6677\n",
      "Epoch 123/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0140 - val_loss: 1.6661\n",
      "Epoch 124/200\n",
      "2952/2952 [==============================] - 38s - loss: 2.0120 - val_loss: 1.6652\n",
      "Epoch 125/200\n",
      "2952/2952 [==============================] - 39s - loss: 2.0112 - val_loss: 1.6665\n",
      "Epoch 126/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0105 - val_loss: 1.6649\n",
      "Epoch 127/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0092 - val_loss: 1.6646\n",
      "Epoch 128/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0090 - val_loss: 1.6656\n",
      "Epoch 129/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0616 - val_loss: 1.6915\n",
      "Epoch 130/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0396 - val_loss: 1.6739\n",
      "Epoch 131/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0199 - val_loss: 1.6683\n",
      "Epoch 132/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0130 - val_loss: 1.6672\n",
      "Epoch 133/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0095 - val_loss: 1.6660\n",
      "Epoch 134/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0070 - val_loss: 1.6648\n",
      "Epoch 135/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0058 - val_loss: 1.6643\n",
      "Epoch 136/200\n",
      "2952/2952 [==============================] - 42s - loss: 2.0050 - val_loss: 1.6637\n",
      "Epoch 137/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0039 - val_loss: 1.6634\n",
      "Epoch 138/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0033 - val_loss: 1.6635\n",
      "Epoch 139/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0021 - val_loss: 1.6632\n",
      "Epoch 140/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0015 - val_loss: 1.6632\n",
      "Epoch 141/200\n",
      "2952/2952 [==============================] - 42s - loss: 2.0006 - val_loss: 1.6631\n",
      "Epoch 142/200\n",
      "2952/2952 [==============================] - 42s - loss: 2.0001 - val_loss: 1.6878\n",
      "Epoch 143/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0045 - val_loss: 1.6653\n",
      "Epoch 144/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0004 - val_loss: 1.6633\n",
      "Epoch 145/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0001 - val_loss: 1.6710\n",
      "Epoch 146/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0025 - val_loss: 1.6964\n",
      "Epoch 147/200\n",
      "2952/2952 [==============================] - 40s - loss: 2.0281 - val_loss: 1.6722\n",
      "Epoch 148/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0047 - val_loss: 1.6644\n",
      "Epoch 149/200\n",
      "2952/2952 [==============================] - 41s - loss: 1.9987 - val_loss: 1.6635\n",
      "Epoch 150/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0055 - val_loss: 1.6803\n",
      "Epoch 151/200\n",
      "2952/2952 [==============================] - 42s - loss: 2.0154 - val_loss: 1.6661\n",
      "Epoch 152/200\n",
      "2952/2952 [==============================] - 41s - loss: 2.0006 - val_loss: 1.6655\n",
      "Epoch 153/200\n",
      "2952/2952 [==============================] - 41s - loss: 1.9962 - val_loss: 1.6628\n",
      "Epoch 154/200\n",
      "2952/2952 [==============================] - 41s - loss: 1.9942 - val_loss: 1.6621\n",
      "Epoch 155/200\n",
      "2952/2952 [==============================] - 41s - loss: 1.9927 - val_loss: 1.6614\n",
      "Epoch 156/200\n",
      "2952/2952 [==============================] - 42s - loss: 1.9929 - val_loss: 1.6617\n",
      "Epoch 157/200\n",
      "2952/2952 [==============================] - 42s - loss: 1.9914 - val_loss: 1.6622\n",
      "Epoch 158/200\n",
      "2952/2952 [==============================] - 40s - loss: 1.9904 - val_loss: 1.6614\n",
      "Epoch 159/200\n",
      "2952/2952 [==============================] - 38s - loss: 1.9901 - val_loss: 1.6619\n",
      "Epoch 160/200\n",
      "2952/2952 [==============================] - 32s - loss: 1.9892 - val_loss: 1.6611\n",
      "Epoch 161/200\n",
      "2952/2952 [==============================] - 31s - loss: 1.9891 - val_loss: 1.6608\n",
      "Epoch 162/200\n",
      "2952/2952 [==============================] - 30s - loss: 1.9876 - val_loss: 1.6616\n",
      "Epoch 163/200\n",
      "2952/2952 [==============================] - 30s - loss: 1.9928 - val_loss: 1.6757\n",
      "Epoch 164/200\n",
      "2952/2952 [==============================] - 33s - loss: 2.0087 - val_loss: 1.6665\n",
      "Epoch 165/200\n",
      "2952/2952 [==============================] - 34s - loss: 1.9922 - val_loss: 1.6645\n",
      "Epoch 166/200\n",
      "2952/2952 [==============================] - 34s - loss: 1.9869 - val_loss: 1.6606\n",
      "Epoch 167/200\n",
      "2952/2952 [==============================] - 39s - loss: 1.9852 - val_loss: 1.6600\n",
      "Epoch 168/200\n",
      "2952/2952 [==============================] - 38s - loss: 1.9844 - val_loss: 1.6612\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2952/2952 [==============================] - 38s - loss: 1.9839 - val_loss: 1.6617\n",
      "Epoch 170/200\n",
      "2952/2952 [==============================] - 38s - loss: 1.9893 - val_loss: 1.6639\n",
      "Epoch 171/200\n",
      "2952/2952 [==============================] - 37s - loss: 1.9944 - val_loss: 1.6656\n",
      "Epoch 172/200\n",
      "2952/2952 [==============================] - 38s - loss: 1.9866 - val_loss: 1.6610\n",
      "Epoch 173/200\n",
      "2952/2952 [==============================] - 37s - loss: 1.9864 - val_loss: 1.6604\n",
      "Epoch 174/200\n",
      "2952/2952 [==============================] - 37s - loss: 1.9821 - val_loss: 1.6608\n",
      "Epoch 175/200\n",
      "2952/2952 [==============================] - 37s - loss: 1.9807 - val_loss: 1.6608\n",
      "Epoch 176/200\n",
      "2952/2952 [==============================] - 37s - loss: 1.9798 - val_loss: 1.6596\n",
      "Epoch 177/200\n",
      "2952/2952 [==============================] - 36s - loss: 1.9792 - val_loss: 1.6605\n",
      "Epoch 178/200\n",
      "2952/2952 [==============================] - 36s - loss: 1.9783 - val_loss: 1.6607\n",
      "Epoch 179/200\n",
      "2952/2952 [==============================] - 38s - loss: 1.9779 - val_loss: 1.6594\n",
      "Epoch 180/200\n",
      "2952/2952 [==============================] - 37s - loss: 1.9779 - val_loss: 1.6589\n",
      "Epoch 181/200\n",
      "2952/2952 [==============================] - 37s - loss: 1.9763 - val_loss: 1.6597\n",
      "Epoch 182/200\n",
      "2952/2952 [==============================] - 40s - loss: 1.9754 - val_loss: 1.6592\n",
      "Epoch 183/200\n",
      "2952/2952 [==============================] - 39s - loss: 1.9748 - val_loss: 1.6598\n",
      "Epoch 184/200\n",
      "2952/2952 [==============================] - 37s - loss: 1.9742 - val_loss: 1.6595\n",
      "Epoch 185/200\n",
      "2952/2952 [==============================] - 36s - loss: 1.9736 - val_loss: 1.6584\n",
      "Epoch 186/200\n",
      "2952/2952 [==============================] - 36s - loss: 1.9750 - val_loss: 1.6594\n",
      "Epoch 187/200\n",
      "2952/2952 [==============================] - 36s - loss: 1.9731 - val_loss: 1.6588\n",
      "Epoch 188/200\n",
      "2952/2952 [==============================] - 37s - loss: 1.9741 - val_loss: 1.6590\n",
      "Epoch 189/200\n",
      "2952/2952 [==============================] - 37s - loss: 1.9718 - val_loss: 1.6596\n",
      "Epoch 190/200\n",
      "2952/2952 [==============================] - 37s - loss: 1.9711 - val_loss: 1.6590\n",
      "Epoch 191/200\n",
      "2952/2952 [==============================] - 38s - loss: 1.9698 - val_loss: 1.6610\n",
      "Epoch 192/200\n",
      "2952/2952 [==============================] - 36s - loss: 1.9730 - val_loss: 1.6583\n",
      "Epoch 193/200\n",
      "2952/2952 [==============================] - 36s - loss: 1.9700 - val_loss: 1.6586\n",
      "Epoch 194/200\n",
      "2952/2952 [==============================] - 37s - loss: 1.9717 - val_loss: 1.6706\n",
      "Epoch 195/200\n",
      "2952/2952 [==============================] - 38s - loss: 1.9835 - val_loss: 1.6643\n",
      "Epoch 196/200\n",
      "2952/2952 [==============================] - 39s - loss: 1.9855 - val_loss: 1.6842\n",
      "Epoch 197/200\n",
      "2952/2952 [==============================] - 39s - loss: 1.9928 - val_loss: 1.6594\n",
      "Epoch 198/200\n",
      "2952/2952 [==============================] - 39s - loss: 1.9750 - val_loss: 1.6598\n",
      "Epoch 199/200\n",
      "2952/2952 [==============================] - 39s - loss: 1.9686 - val_loss: 1.6584\n",
      "Epoch 200/200\n",
      "2952/2952 [==============================] - 41s - loss: 1.9673 - val_loss: 1.6597\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128  # Batch size for training.\n",
    "epochs = 200  # Number of epochs to train for.\n",
    "latent_dim = 32  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Callbacks\n",
    "checkpointer = ModelCheckpoint('checkpoints/encoder-decoder-model.hdf5', \n",
    "                               verbose=0, save_weights_only=False)\n",
    "\n",
    "# Run training\n",
    "sgd = optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adadlt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size, callbacks = [checkpointer],\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "\n",
    "# save encoder model\n",
    "encoder_model_yaml = encoder_model.to_yaml()\n",
    "with open(\"encoder_model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(encoder_model_yaml)\n",
    "encoder_model.save('encoder_model.h5')\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "513px",
    "left": "1498px",
    "right": "20px",
    "top": "116px",
    "width": "352px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
