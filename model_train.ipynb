{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of samples:', 8134)\n",
      "('Number of unique input tokens:', 21)\n",
      "('Number of unique output classes:', 6)\n",
      "('Max sequence length for inputs:', 750)\n",
      "('Max sequence length for outputs:', 1)\n",
      "('Class counts:', [('1', 1596), ('3', 2225), ('2', 1660), ('5', 539), ('4', 1033), ('6', 1081)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acar/.virtualenvs/gpuenv/lib/python2.7/site-packages/ipykernel_launcher.py:75: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(16, return_sequences=True, kernel_regularizer=<keras.reg...)`\n",
      "/home/acar/.virtualenvs/gpuenv/lib/python2.7/site-packages/ipykernel_launcher.py:77: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(16, return_sequences=False, kernel_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 21)          0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None, 21)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 32)          4864      \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 13,638\n",
      "Trainable params: 13,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6507 samples, validate on 1627 samples\n",
      "Epoch 1/50\n",
      "6507/6507 [==============================] - 3136s - loss: 1.5837 - acc: 0.3498 - val_loss: 4.4532 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "6507/6507 [==============================] - 3169s - loss: 1.2856 - acc: 0.4750 - val_loss: 5.3367 - val_acc: 0.0541\n",
      "Epoch 3/50\n",
      "6507/6507 [==============================] - 3217s - loss: 1.1592 - acc: 0.5348 - val_loss: 7.0292 - val_acc: 0.0424\n",
      "Epoch 4/50\n",
      "6507/6507 [==============================] - 3210s - loss: 1.0830 - acc: 0.5729 - val_loss: 8.0409 - val_acc: 0.0516\n",
      "Epoch 5/50\n",
      "6507/6507 [==============================] - 3211s - loss: 1.0220 - acc: 0.5995 - val_loss: 8.9321 - val_acc: 0.0762\n",
      "Epoch 6/50\n",
      "6507/6507 [==============================] - 3216s - loss: 0.9745 - acc: 0.6264 - val_loss: 8.5162 - val_acc: 0.1205\n",
      "Epoch 7/50\n",
      "6507/6507 [==============================] - 3215s - loss: 0.9320 - acc: 0.6536 - val_loss: 9.1105 - val_acc: 0.1887\n",
      "Epoch 8/50\n",
      "6507/6507 [==============================] - 3210s - loss: 0.8971 - acc: 0.6653 - val_loss: 9.7852 - val_acc: 0.0873\n",
      "Epoch 9/50\n",
      "6507/6507 [==============================] - 3214s - loss: 0.8677 - acc: 0.6782 - val_loss: 10.1438 - val_acc: 0.1967\n",
      "Epoch 10/50\n",
      "6507/6507 [==============================] - 3176s - loss: 0.8379 - acc: 0.6917 - val_loss: 10.1291 - val_acc: 0.1623\n",
      "Epoch 11/50\n",
      "6507/6507 [==============================] - 3152s - loss: 0.8219 - acc: 0.6922 - val_loss: 10.0552 - val_acc: 0.1604\n",
      "Epoch 12/50\n",
      "6507/6507 [==============================] - 3157s - loss: 0.7927 - acc: 0.7131 - val_loss: 10.2449 - val_acc: 0.1604\n",
      "Epoch 13/50\n",
      "6507/6507 [==============================] - 3148s - loss: 0.7762 - acc: 0.7154 - val_loss: 10.3002 - val_acc: 0.2188\n",
      "Epoch 14/50\n",
      "6507/6507 [==============================] - 3143s - loss: 0.7677 - acc: 0.7234 - val_loss: 10.2094 - val_acc: 0.1469\n",
      "Epoch 15/50\n",
      "6507/6507 [==============================] - 3685s - loss: 0.7569 - acc: 0.7298 - val_loss: 10.1290 - val_acc: 0.1739\n",
      "Epoch 16/50\n",
      " 800/6507 [==>...........................] - ETA: 2574s - loss: 0.7390 - acc: 0.7425"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Masking, Bidirectional\n",
    "from keras.regularizers import l2, l1\n",
    "import numpy as np\n",
    "from readdata import read_data\n",
    "import collections\n",
    "\n",
    "x_train, y_train, x_text, y_test = read_data(level=0, length_limit=750)\n",
    "# x_train = x_train * 5\n",
    "# y_train = y_train * 5\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "targets = set()\n",
    "for i, (input_text, target) in enumerate(zip(x_train, y_train)):\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    if target not in targets:\n",
    "        targets.add(target)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "targets = sorted(list(targets))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "nb_targets = len(targets)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output classes:', nb_targets)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "print('Class counts:', [(key, value) for\n",
    "                        key, value in (collections.Counter(y_train)).iteritems()])\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(targets)])\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "y_encoded = np.zeros((len(input_texts), nb_targets), dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, -(t+1), input_token_index[char]] = 1.\n",
    "    y_encoded[i, target_token_index[target_text]] = 1.\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 16  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 16  # Latent dimensionality of the encoding space.\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "mask_1 = Masking(mask_value=0.0)(encoder_inputs)\n",
    "encoder_1 = Bidirectional(LSTM(latent_dim, return_sequences=True,  \n",
    "                               W_regularizer=l2(0.004)))(mask_1)\n",
    "encoder_2 = Bidirectional(LSTM(latent_dim, return_sequences=False,  \n",
    "                               W_regularizer=l2(0.004)))(encoder_1)\n",
    "# encoder_2 = LSTM(latent_dim*2,  W_regularizer=l2(0.001), recurrent_dropout=0.3)(encoder_1)\n",
    "# encoder_states = [state_h, state_c]\n",
    "dense_1 = Dense(64, activation='relu')(encoder_2)\n",
    "dropout_1 = Dropout(0.50)(dense_1)\n",
    "# dense_2 = Dense(128, activation='relu')(dropout_1)\n",
    "# dropout_2 = Dropout(0.50)(dense_1)\n",
    "dense_outputs = Dense(nb_targets, activation='softmax')(dropout_1)\n",
    "\n",
    "model = Model(encoder_inputs, dense_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(encoder_input_data, y_encoded,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,\n",
    "          verbose=1)\n",
    "\n",
    "a = 5\n",
    "print \"end\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
