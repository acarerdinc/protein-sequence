{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of samples:', 8807)\n",
      "('Number of unique input tokens:', 21)\n",
      "('Number of unique output classes:', 6)\n",
      "('Max sequence length for inputs:', 1907)\n",
      "('Max sequence length for outputs:', 1)\n",
      "('Class counts:', [('1', 1654), ('3', 2652), ('2', 1716), ('5', 546), ('4', 1151), ('6', 1088)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acar/.virtualenvs/gpuenv/lib/python2.7/site-packages/ipykernel_launcher.py:75: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(16, return_sequences=True, kernel_regularizer=<keras.reg...)`\n",
      "/home/acar/.virtualenvs/gpuenv/lib/python2.7/site-packages/ipykernel_launcher.py:77: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(16, return_sequences=False, kernel_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, None, 21)          0         \n",
      "_________________________________________________________________\n",
      "masking_5 (Masking)          (None, None, 21)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, None, 32)          4864      \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 13,638\n",
      "Trainable params: 13,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7045 samples, validate on 1762 samples\n",
      "Epoch 1/50\n",
      "  16/7045 [..............................] - ETA: 14967s - loss: 2.0807 - acc: 0.2500"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Masking, Bidirectional\n",
    "from keras.regularizers import l2, l1\n",
    "import numpy as np\n",
    "from readdata import read_data\n",
    "import collections\n",
    "\n",
    "x_train, y_train, x_text, y_test = read_data(level=0, length_limit=2000)\n",
    "# x_train = x_train * 5\n",
    "# y_train = y_train * 5\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "targets = set()\n",
    "for i, (input_text, target) in enumerate(zip(x_train, y_train)):\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    if target not in targets:\n",
    "        targets.add(target)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "targets = sorted(list(targets))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "nb_targets = len(targets)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output classes:', nb_targets)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "print('Class counts:', [(key, value) for\n",
    "                        key, value in (collections.Counter(y_train)).iteritems()])\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(targets)])\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "y_encoded = np.zeros((len(input_texts), nb_targets), dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, -(t+1), input_token_index[char]] = 1.\n",
    "    y_encoded[i, target_token_index[target_text]] = 1.\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 16  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 16  # Latent dimensionality of the encoding space.\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "mask_1 = Masking(mask_value=0.0)(encoder_inputs)\n",
    "encoder_1 = Bidirectional(LSTM(latent_dim, return_sequences=True,  \n",
    "                               W_regularizer=l2(0.004)))(mask_1)\n",
    "encoder_2 = Bidirectional(LSTM(latent_dim, return_sequences=False,  \n",
    "                               W_regularizer=l2(0.004)))(encoder_1)\n",
    "# encoder_2 = LSTM(latent_dim*2,  W_regularizer=l2(0.001), recurrent_dropout=0.3)(encoder_1)\n",
    "# encoder_states = [state_h, state_c]\n",
    "dense_1 = Dense(64, activation='relu')(encoder_2)\n",
    "dropout_1 = Dropout(0.50)(dense_1)\n",
    "# dense_2 = Dense(128, activation='relu')(dropout_1)\n",
    "# dropout_2 = Dropout(0.50)(dense_1)\n",
    "dense_outputs = Dense(nb_targets, activation='softmax')(dropout_1)\n",
    "\n",
    "model = Model(encoder_inputs, dense_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(encoder_input_data, y_encoded,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,\n",
    "          verbose=1)\n",
    "\n",
    "a = 5\n",
    "print \"end\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
